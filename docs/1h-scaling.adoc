// JBoss, Home of Professional Open Source
// Copyright 2016, Red Hat, Inc. and/or its affiliates, and individual
// contributors by the @authors tag. See the copyright.txt in the
// distribution for a full listing of individual contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
// http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

### Scaling In and Out
Duration: 5:00

Scaling the number of replicas of our Hello World service is as simple as running :

[source,subs="normal,attributes"]
----
$ *kubectl scale deployment helloworld-service-vertx --replicas=4*
----

You can very quickly see that the replication controller has been updated:

[source,subs="normal,attributes"]
----
$ *kubectl get deployment*
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
frontend-ui                2         2         2            2           11h
guestbook-service          2         2         2            2           20h
helloworld-service-vertx   4         4         4            4           20h
mysql                      1         1         1            1           16h

$ *kubectl get pods*
NAME                              READY     STATUS    RESTARTS   AGE
frontend-ui-...                     1/1       Running   0          11h
frontend-ui-...                     1/1       Running   0          11h
guestbook-service-...               1/1       Running   0          12h
guestbook-service-...               1/1       Running   0          11h
helloworld-service-vertx-...        1/1       Running   0          12h
helloworld-service-vertx-...        1/1       Running   0          1m
helloworld-service-vertx-...        1/1       Running   0          11h
helloworld-service-vertx-...        1/1       Running   0          1m
mysql-...                           1/1       Running   0          16h
----

Let's scale out even more!
[source,subs="normal,attributes"]
----
$ *kubectl scale deployment helloworld-service-vertx --replicas=15*
----

Let’s take a look at the status of the pods:

[source,subs="normal,attributes"]
----
$ *kubectl get pods*
NAME                                        READY     STATUS    RESTARTS   AGE
frontend-ui-658026717-i5b0v                 1/1       Running   0          11h
frontend-ui-658026717-kv8sv                 1/1       Running   0          11h
guestbook-service-1620957573-cjf4f          1/1       Running   0          12h
guestbook-service-1620957573-hucws          1/1       Running   0          11h
*helloworld-service-vertx-3558582262-0cx1k   0/1       Pending   0          6s*
helloworld-service-vertx-3558582262-0gy3z   1/1       Running   0          1m
*helloworld-service-vertx-3558582262-1b40u   0/1       Pending   0          6s*
*helloworld-service-vertx-3558582262-1m3b2   0/1       Pending   0          1m*
helloworld-service-vertx-3558582262-5nmfg   1/1       Running   0          1m
helloworld-service-vertx-3558582262-7noao   1/1       Running   0          12h
helloworld-service-vertx-3558582262-979ln   1/1       Running   0          6m
helloworld-service-vertx-3558582262-9q523   1/1       Running   0          1m
helloworld-service-vertx-3558582262-c7c74   1/1       Running   0          1m
helloworld-service-vertx-3558582262-cmlz2   1/1       Running   0          1m
helloworld-service-vertx-3558582262-nauta   1/1       Running   0          1m
helloworld-service-vertx-3558582262-oql4g   1/1       Running   0          11h
helloworld-service-vertx-3558582262-rermx   1/1       Running   0          6m
helloworld-service-vertx-3558582262-txulx   1/1       Running   0          1m
*helloworld-service-vertx-3558582262-xu68l   0/1       Pending   0          6s*
mysql-2668656853-do6wa                      1/1       Running   0          16h
----

Oh no! Some of the pods are in the _Pending_ state!  That is because we only have four physical nodes, and the underlying infrastructure has run out of capacity to run the containers with the requested resources.

Pick a Pod name that is associated with the Pending state to confirm the lack of resources in the detailed status:

[source,subs="normal,attributes"]
----
$ *kubectl describe pod helloworld-service-...*
Name:         helloworld-service-vertx-...
Namespace:    default
Node:         /
Labels:       app=helloworld-service-vertx,pod-template-hash=3558582262,version=1.0,visualize=true
Status:       Pending
...
Events:
  FirstSeen LastSeen  Count	From                    Type      Reason      Message
  ---------	--------	----- ----                    --------  ------      -------
  3m        1m        2	    {default-scheduler }    Warning		FailedScheduling  pod (helloworld-service-vertx-3558582262-1m3b2) failed to fit in any node
fit failure on node (gke-guestbook-default-pool-a27323b1-5xj3): Node didn't have enough resource: CPU, requested: 100, used: 910, capacity: 1000
fit failure on node (gke-guestbook-default-pool-a27323b1-f8l3): Node didn't have enough resource: CPU, requested: 100, used: 960, capacity: 1000
fit failure on node (gke-guestbook-default-pool-a27323b1-pzt9): Node didn't have enough resource: CPU, requested: 100, used: 1000, capacity: 1000
fit failure on node (gke-guestbook-default-pool-a27323b1-2vqp): Node didn't have enough resource: CPU, requested: 100, used: 1000, capacity: 1000
----

The good news is that we can easily spin up another Compute Engine instance to append to the cluster.
First, find the Compute Engine Instance Group that’s managing the Kubernetes nodes (the name is prefixed with _"gke-"_):

[source,subs="normal,attributes"]
----
$ *gcloud compute instance-groups list*
NAME                                     ZONE           NETWORK  MANAGED  INSTANCES
gke-guestbook-default-pool-a27323b1-grp  us-central1-b  default  Yes      4
----

You can resize the number of nodes by updating the Instance Group size:

[source,subs="normal,attributes"]
----
$ *gcloud compute instance-groups managed resize gke-guestbook-default-pool-a27323b1-grp --size 5*
Updated [https://www.googleapis.com/compute/v1/projects/feisty-parity-130103/zones/us-central1-b/instanceGroupManagers/gke-guestbook-default-pool-a27323b1-grp].
---
baseInstanceName: gke-guestbook-default-pool-a27323b1
creationTimestamp: '2016-05-25T08:15:27.864-07:00'
currentActions:
  abandoning: 0
  creating: 1
  deleting: 0
...
----

You can see a new Compute Engine instance is starting:

[source,subs="normal,attributes"]
----
$ *gcloud compute instances list*
...
----

Once the new instance has joined the Kubernetes cluster, you’ll should be able to see it with this command:

[source,subs="normal,attributes"]
----
$ *kubectl get nodes*
NAME                                       STATUS    AGE
gke-guestbook-default-pool-a27323b1-2vqp   Ready     7d
gke-guestbook-default-pool-a27323b1-5xj3   Ready     7d
gke-guestbook-default-pool-a27323b1-f8l3   Ready     7d
gke-guestbook-default-pool-a27323b1-ktcw   Ready     1m
gke-guestbook-default-pool-a27323b1-pzt9   Ready     7d
----

Use _kubectl get pods_ to see the that the _Pending_ pods have been scheduled.

Once you see they are scheduled, reduce the number of replicas back to 4 so that we can free up resources for the later labs:

[source,subs="normal,attributes"]
----
$ *kubectl scale deployment helloworld-service-vertx --replicas=4*
----
